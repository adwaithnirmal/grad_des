{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uscmlsystems/ml-systems-hw1-madhavdanturthi/blob/main/2_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUKXxT4DGUJn"
      },
      "source": [
        "# EE 508 HW 1 Part 2: Classification\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlMLMUKbGUJ0"
      },
      "source": [
        "## Cross Validation, Bias-Variance trade-off, Overfitting\n",
        "\n",
        "In this section, we will demonstrate data splitting and the validation process in machine learning paradigms. We will use the Iris dataset from the `sklearn` library.\n",
        "\n",
        "Objective:\n",
        "- Train a Fully-Connected Network (FCN) for classification.  \n",
        "- Partition the data using three-fold cross-validation and report the training, validation, and testing accuracy.  \n",
        "- Train the model using cross-entropy loss and evaluate it with 0/1 loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApMuFMivGUJ1"
      },
      "outputs": [],
      "source": [
        "# import required libraries and dataset\n",
        "import numpy as np\n",
        "# load sklearn for ML functions\n",
        "from sklearn.datasets import load_iris\n",
        "# load torch dataaset for training NNs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(['ggplot'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8MkEfc-GUJ2"
      },
      "source": [
        "### **TODO 1**: Implement the cross validation function\n",
        "In this function, the dataset is first shuffled. Then, we need to implement a loop that iterates through each fold, selecting a subset of samples as the validation set while assigning the remaining samples to the training set, and stores these partitions in the `folds` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN6hh3RPGUJ2"
      },
      "outputs": [],
      "source": [
        "def cross_validation(x: np.array, y: np.array, n_folds: int=3):\n",
        "    \"\"\"\n",
        "    Splitting the dataset to the given fold\n",
        "    Parameters:\n",
        "    - x: Feaures of the dataset, with shape (n_samples, n_features)\n",
        "    - y: Class label of the dataset, with shape (n_samples,)\n",
        "    - n_folds: the given number of partitions\n",
        "        For instnace, 5-fold CV with 100 percentage:\n",
        "        fold_1: training on 20~99, validation on 0~19(%)\n",
        "        fold_2: training on 0~19 and 40~99, validation on 20~39(%)\n",
        "        fold_3: training on 0~39 and 60~99, validation on 40~59(%)\n",
        "        fold_4: training on 0~59 and 80~99, validation on 60~79(%)\n",
        "        fold_5: training on 0~79, validation on 80~99(%)\n",
        "\n",
        "    Returns:\n",
        "    - folds (list): In the format with len(folds) == n_folds\n",
        "        [\n",
        "            (x_train_fold1, y_train_fold1, x_valid_fold1, y_valid_fold1),\n",
        "            (x_train_fold2, y_train_fold2, x_valid_fold2, y_valid_fold2),\n",
        "            (x_train_fold3, y_train_fold3, x_valid_fold3, y_valid_fold3),\n",
        "            ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "\n",
        "    folds = []\n",
        "    n_data = x.shape[0]\n",
        "    index = np.arange(n_data)\n",
        "    # shaffle the data with np.random.shuffle\n",
        "    np.random.shuffle(index)\n",
        "    # find the partition with numpy.linspace\n",
        "    partitions = np.linspace(0, n_data, num=n_folds+1, endpoint=True)\n",
        "    partitions = partitions.astype(int)\n",
        "\n",
        "    # Finish the code here\n",
        "    for i in range(n_folds):\n",
        "      testIdx = index[partitions[i]:partitions[i+1]]\n",
        "      trainIdx = np.concatenate([index[:partitions[i]], index[partitions[i+1]:]])\n",
        "      folds.append((x[trainIdx], y[trainIdx], x[testIdx], y[testIdx]))\n",
        "\n",
        "    print(f\"The Partitions:\")\n",
        "    for idx, (_, train_y, _, valid_y) in enumerate(folds):\n",
        "        print(f\"[Fold-{idx+1}] #Training: {train_y.shape[0]:4>0d}; #Validation: {valid_y.shape[0]:4>0d}\")\n",
        "        from collections import Counter\n",
        "        # you check check the label distribution\n",
        "        # print(Counter(train_y))\n",
        "        # print(Counter(valid_y))\n",
        "\n",
        "    return folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_qaH5J9GUJ2",
        "outputId": "91d40fcf-159f-4da6-c373-191be9f63db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Partitions:\n",
            "[Fold-1] #Training: 100; #Validation: 50\n",
            "[Fold-2] #Training: 100; #Validation: 50\n",
            "[Fold-3] #Training: 100; #Validation: 50\n"
          ]
        }
      ],
      "source": [
        "# fixed the random seed\n",
        "np.random.seed(42)\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "x, y = iris.data, iris.target\n",
        "# Split into training and testing sets\n",
        "three_folds = cross_validation(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeD8EE4hGUJ2"
      },
      "source": [
        "### **TODO 2**: Build a Fully-Connect Networks with PyTorch\n",
        "In this section, we build simple FCN models with different numbers of hidden units for the classification task.\n",
        "\n",
        "- **Training:** Use cross-entropy for optimization.  \n",
        "- **Inferencing:** Evaluate with 0/1 loss.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eNaDWkhGUJ2"
      },
      "outputs": [],
      "source": [
        "# define the FCN model\n",
        "class FCN_model(nn.Module):\n",
        "    # take the argument for the number of hidden units\n",
        "    def __init__(self, n_hidden=32):\n",
        "        # Finish the code here\n",
        "        super(FCN_model, self).__init__()\n",
        "        self.inputSize = x.shape[1]\n",
        "        self.classes = np.unique(y).shape[0]\n",
        "        self.hidden1 = nn.Linear(self.inputSize, n_hidden)\n",
        "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(n_hidden, self.classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Finish the code here\n",
        "        x = self.hidden1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9UIibs5GUJ3"
      },
      "source": [
        "Set up the evaluation and training functions for the FCN models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksNrJ1FUGUJ3"
      },
      "outputs": [],
      "source": [
        "def eval(model:nn.Module,\n",
        "         x:torch.tensor,\n",
        "         y:torch.tensor) -> float:\n",
        "    \"\"\"Evaluate the model: inference the model with 0/1 loss\n",
        "    We can define the output label is the maximum logit from the model\n",
        "\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x: input features\n",
        "    - y: ground truth labels, dtype=long\n",
        "\n",
        "    Returns:\n",
        "    - loss: the average 0/1 loss value\n",
        "    \"\"\"\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = torch.argmax(model(x), dim=1)\n",
        "\n",
        "    loss = 0\n",
        "    # Finish the code here\n",
        "    for i in range(preds.shape[0]):\n",
        "      if preds[i] != y[i]:\n",
        "        loss += 1\n",
        "\n",
        "    print(f\"Averaging 0/1 loss: {loss/preds.shape[0]:.4f}\")\n",
        "    return loss/preds.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqsPm14-GUJ3"
      },
      "outputs": [],
      "source": [
        "def train(model:nn.Module,\n",
        "          x_train:torch.tensor,\n",
        "          y_train:torch.tensor,\n",
        "          x_valid:torch.tensor,\n",
        "          y_valid:torch.tensor,\n",
        "          epochs:int=300):\n",
        "    \"\"\"Trining process\n",
        "    Parameters:\n",
        "    - model: the FCN model\n",
        "    - x_train, y_train: trainig features and labels (dtype=long)\n",
        "    - x_valid, y_valid: validation features and labels (dtype=long)\n",
        "    - epochs: number of the epoches for training\n",
        "    \"\"\"\n",
        "    # To simplify the process\n",
        "    # we do not take batches but use all the training samples\n",
        "    # set up the objective function and the optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "    # training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        # Forward pass\n",
        "        outputs = model(x_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Cross Entropy Loss: {loss.item():.4f}\")\n",
        "            print(f\"[Train] \", end=\"\")\n",
        "            eval(model, x_train, y_train)\n",
        "            print(f\"[Valid] \", end=\"\")\n",
        "            eval(model, x_valid, y_valid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QMhFLTsGUJ3"
      },
      "source": [
        "### **TODO 3**: Conduct the training/validation process in each fold\n",
        "We will use three-fold validation, meaning you need to train three models and report the training and validation loss for all three folds.  \n",
        "\n",
        "First, instantiate an FCN model with 32 hidden units.  \n",
        "Then, call the `train` function, which takes the training and validation folds created by the `cross_validation()` function, along with the model, as input. Set `epochs` to `500`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKR0PxrjGUJ3",
        "outputId": "9c995df4-d2bd-4f5f-e52d-ca9d394ea1e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Traing Fold 0 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.7262\n",
            "[Train] Averaging 0/1 loss: 0.2800\n",
            "[Valid] Averaging 0/1 loss: 0.2800\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4748\n",
            "[Train] Averaging 0/1 loss: 0.0800\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Epoch [300/500], Cross Entropy Loss: 0.3668\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [400/500], Cross Entropy Loss: 0.2840\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Epoch [500/500], Cross Entropy Loss: 0.2161\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Averaging 0/1 loss: 0.0300\n",
            "Averaging 0/1 loss: 0.0000\n",
            "===== Traing Fold 1 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.7441\n",
            "[Train] Averaging 0/1 loss: 0.3300\n",
            "[Valid] Averaging 0/1 loss: 0.3400\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4805\n",
            "[Train] Averaging 0/1 loss: 0.1300\n",
            "[Valid] Averaging 0/1 loss: 0.1600\n",
            "Epoch [300/500], Cross Entropy Loss: 0.3527\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.1000\n",
            "Epoch [400/500], Cross Entropy Loss: 0.2593\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [500/500], Cross Entropy Loss: 0.1933\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0600\n",
            "===== Traing Fold 2 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.7034\n",
            "[Train] Averaging 0/1 loss: 0.3100\n",
            "[Valid] Averaging 0/1 loss: 0.3600\n",
            "Epoch [200/500], Cross Entropy Loss: 0.4511\n",
            "[Train] Averaging 0/1 loss: 0.1300\n",
            "[Valid] Averaging 0/1 loss: 0.1000\n",
            "Epoch [300/500], Cross Entropy Loss: 0.3532\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0400\n",
            "Epoch [400/500], Cross Entropy Loss: 0.2850\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [500/500], Cross Entropy Loss: 0.2311\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0200\n"
          ]
        }
      ],
      "source": [
        "train_losses, valid_losses = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Traing Fold {idx} =====\")\n",
        "    x_train = torch.Tensor(x_train)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    x_valid = torch.Tensor(x_valid)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "\n",
        "    # Finish the code here\n",
        "    model = FCN_model(n_hidden=32)\n",
        "    train(model, x_train, y_train, x_valid, y_valid, 500)\n",
        "\n",
        "\n",
        "    train_losses.append(eval(model, x_train, y_train))\n",
        "    valid_losses.append(eval(model, x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDp-WRM1GUJ3",
        "outputId": "38524123-beed-4c5e-e5be-94c29be0407a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.03,            0.00\n",
            "    1,          0.02,            0.06\n",
            "    2,          0.02,            0.02\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_losses, valid_losses)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVCyyIGNGUJ4"
      },
      "source": [
        "### **TODO4**: Check over-fitting with complex model\n",
        "We can follow the same procedure with a more complex FCN model.  \n",
        "Now, set the `number of hidden units` to `2048` and repeat the process for three-fold validation with `epochs = 500`.  \n",
        "The gap between the training and validation performance should increase.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8waqhH1VGUJ4",
        "outputId": "a1f76965-02a9-43cd-a160-fd0497e852b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Traing Fold 0 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.3343\n",
            "[Train] Averaging 0/1 loss: 0.2200\n",
            "[Valid] Averaging 0/1 loss: 0.1800\n",
            "Epoch [200/500], Cross Entropy Loss: 0.2298\n",
            "[Train] Averaging 0/1 loss: 0.1300\n",
            "[Valid] Averaging 0/1 loss: 0.0800\n",
            "Epoch [300/500], Cross Entropy Loss: 0.1309\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [400/500], Cross Entropy Loss: 0.0885\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Epoch [500/500], Cross Entropy Loss: 0.0805\n",
            "[Train] Averaging 0/1 loss: 0.0200\n",
            "[Valid] Averaging 0/1 loss: 0.0000\n",
            "Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0000\n",
            "===== Traing Fold 1 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.3159\n",
            "[Train] Averaging 0/1 loss: 0.2200\n",
            "[Valid] Averaging 0/1 loss: 0.2400\n",
            "Epoch [200/500], Cross Entropy Loss: 0.2000\n",
            "[Train] Averaging 0/1 loss: 0.1200\n",
            "[Valid] Averaging 0/1 loss: 0.1600\n",
            "Epoch [300/500], Cross Entropy Loss: 0.0913\n",
            "[Train] Averaging 0/1 loss: 0.0300\n",
            "[Valid] Averaging 0/1 loss: 0.1000\n",
            "Epoch [400/500], Cross Entropy Loss: 0.0534\n",
            "[Train] Averaging 0/1 loss: 0.0100\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [500/500], Cross Entropy Loss: 0.0424\n",
            "[Train] Averaging 0/1 loss: 0.0100\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Averaging 0/1 loss: 0.0100\n",
            "Averaging 0/1 loss: 0.0600\n",
            "===== Traing Fold 2 =====\n",
            "Epoch [100/500], Cross Entropy Loss: 0.2899\n",
            "[Train] Averaging 0/1 loss: 0.1800\n",
            "[Valid] Averaging 0/1 loss: 0.1800\n",
            "Epoch [200/500], Cross Entropy Loss: 0.2008\n",
            "[Train] Averaging 0/1 loss: 0.1000\n",
            "[Valid] Averaging 0/1 loss: 0.0600\n",
            "Epoch [300/500], Cross Entropy Loss: 0.1425\n",
            "[Train] Averaging 0/1 loss: 0.0600\n",
            "[Valid] Averaging 0/1 loss: 0.0400\n",
            "Epoch [400/500], Cross Entropy Loss: 0.1188\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Epoch [500/500], Cross Entropy Loss: 0.1072\n",
            "[Train] Averaging 0/1 loss: 0.0400\n",
            "[Valid] Averaging 0/1 loss: 0.0200\n",
            "Averaging 0/1 loss: 0.0400\n",
            "Averaging 0/1 loss: 0.0200\n"
          ]
        }
      ],
      "source": [
        "train_overfit, valid_overfit = [], []\n",
        "\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "    print(f\"===== Traing Fold {idx} =====\")\n",
        "    x_train = torch.Tensor(x_train)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    x_valid = torch.Tensor(x_valid)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "\n",
        "    # Finish the code here\n",
        "    model = FCN_model(n_hidden=2048)\n",
        "    train(model, x_train, y_train, x_valid, y_valid, 500)\n",
        "\n",
        "\n",
        "    train_overfit.append(eval(model, x_train, y_train))\n",
        "    valid_overfit.append(eval(model, x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWbUEc23GUJ4",
        "outputId": "9cd6010b-1fd7-4934-d19b-c8f76887e4af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.02,            0.00\n",
            "    1,          0.01,            0.06\n",
            "    2,          0.04,            0.02\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_overfit, valid_overfit)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsHsa7TCGUJ4"
      },
      "source": [
        "### **TODO 5**: Compare the FCN with statistical ML models\n",
        "Here, we will use the Naive Bayes model from the `sklearn` library and perform three-fold validation.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIkOshZqGUJ4"
      },
      "outputs": [],
      "source": [
        "# Load the Naive Bayes classifier from the library\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "train_nb, valid_nb = [], []\n",
        "for idx, (x_train, y_train, x_valid, y_valid) in enumerate(three_folds):\n",
        "\n",
        "    # Finish the code here\n",
        "    nb = GaussianNB()\n",
        "    nb.fit(x_train, y_train)\n",
        "    train_acc = nb.score(x_train, y_train)\n",
        "    valid_acc = nb.score(x_valid, y_valid)\n",
        "\n",
        "\n",
        "    train_nb.append(1 - train_acc)\n",
        "    valid_nb.append(1 - valid_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abZX_pMaGUJ5",
        "outputId": "0a9f35db-ed1e-4832-99ab-8ccb24195368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fold, training loss, validation loss\n",
            "    0,          0.05,            0.04\n",
            "    1,          0.02,            0.06\n",
            "    2,          0.04,            0.04\n"
          ]
        }
      ],
      "source": [
        "print(f\"#Fold, training loss, validation loss\")\n",
        "for idx, (train_loss, valid_loss) in enumerate(zip(train_nb, valid_nb)):\n",
        "    print(f\"{idx:>5d},          {train_loss:.2f},            {valid_loss:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn6wbJUIGUJ5"
      },
      "source": [
        "### **TODO 6**:\n",
        "Answer the following questions in the next cell.  \n",
        "1. What is the the bias-variance trade-off in machine learning?\n",
        "2. How to reduce overfitting and underfitting?\n",
        "3. How do the training and inference processes differ between the Naive Bayes model and a fully connected neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY7C8XTsGUJ5"
      },
      "source": [
        "Your anwser:\n",
        "1. The bias-variance trade-off is essentially that high-bias can lead to underfitting while high-variance can lead to overfitting. Therefore, it is important to balance both so that there is no underfitting or overfitting and the model is able to generalize well without error due to variance or error due to bias.\n",
        "\n",
        "2. You can reduce overfitting by using a simpler model which is not too complex or by increasing the training set size. This ensures that there are not very simple assumptions being made. The most common way to reduce overfitting however is to use regularization by adding a penalty to the loss function, which discourages large model weights and improves generalizations to unseen data. By limiting the magnitude of the model parameters, the model becomes less sensitive to small noises, reducing variance and thereby reducing overfitting. For underfitting, you can increase the model complexitiy and also train the model for longer. You can also reduce regularization if the limits are too strong.\n",
        "\n",
        "3. For Naive Bayes, training is done based on using Bayes theorem to compute probabilities for features while assuming conditional independence given the classification. Inference is done by using the computed probabiliites to classify new data. On the other hand, a fully connected neural network uses backpropagation and gradient descent to update weights through iterations for training, making it a very computationally expensive process. For inference, forward propagation is used through multiple hidden layers and activation functions which also makes it slower than Naive Bayes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "caption",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "undefined.undefined.undefined"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}